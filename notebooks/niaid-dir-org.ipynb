{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NIAID DIR Laboratory Descriptions Data Set\n",
    "\n",
    "This notebook creates a data set of the National Institute of Allergy or Infectious Diseases (NIAID) Division of Intramural Research (DIR) organizational structure. The data set was created for later consumption into a Neo4j graph representation and subsequently integrated into the PubMed Knowledge Graph (PKG) for further analyses. To that end, the script outputs a CSV. In order to obtain the data, this notebook utilizes web scraping directly from the information found in the NIAID DIR Laboratory Descriptions. It includes the following features of substantive interest.\n",
    "\n",
    "_**Date:** Jan 2022_\n",
    "\n",
    "_**Contact:** Nick Kunz, Deloitte Consulting LLP (nkunz@deloitte.com)_\n",
    "\n",
    "\n",
    "### Data Features\n",
    "1. **Name** (object): Name of the principal investigators, staff scientists, and staff clinicians. There are 180 unique names.\n",
    "2.  **Education** (object): Research and training credentials for each name. There are 15 unique credentials.\n",
    "3. **Branch** (object): The branch name nested within the DIR. There are 20 unique branches.\n",
    "4. **Section** (object): The section/unit name nested within its respective branch. There are 162 unique sections."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Depdendencies\n",
    "1. **Python** (3.8.2): Language\n",
    "2. **Pandas** (1.2.4): Dataframes for data manipulation\n",
    "3. **Requests** (2.26.0): Web requests for naviation\n",
    "3. **Selenium** (3.141.0): Framework for web automation\n",
    "4. **TQDM** (4.61.2): Progress bar for job status and completion\n",
    "5. **GeckoDriver** (0.30.0): WebDriver utilized in Selenium \n",
    "\n",
    "    _Note: Notebook only supports GeckoDriver auto download on Windows machines. If you are running Linux or Mac, you will need to download GeckoDriver (https://github.com/mozilla/geckodriver/releases) and specify its excutable path manually. Also, requires Python kernel to accept web requests._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "## libraries\n",
    "import io\n",
    "import zipfile as zp\n",
    "import requests as rq\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver import FirefoxProfile\n",
    "from selenium.webdriver import FirefoxOptions\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dependency Downloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "## get geckodriver\n",
    "def gecko_downloader(os = 'win64', path = './'):\n",
    "\n",
    "    \"\"\"\n",
    "    Desc:\n",
    "        Downloads and extracts GeckoDriver required for Selenium WebDriver.\n",
    "    \n",
    "    Args:\n",
    "        os (str): Specify OS on local machine (takes \"win64\" or \"win32\").\n",
    "        path (str): File path for saving GeckoDriver on local machine.\n",
    "\n",
    "    Returns:\n",
    "        None (saves file 'geckodriver.exe' to specified path in 'path').\n",
    "    \n",
    "    Raises:\n",
    "        ValueError: Cannot download GeckoDriver\n",
    "    \"\"\"\n",
    "\n",
    "    ## arg quality\n",
    "    if type(os) is not str:\n",
    "        raise TypeError('os arg requires str.')\n",
    "\n",
    "    if os != 'win64' and os != 'win32':\n",
    "        raise TypeError('os arg requires valid input: \"win64\", \"win32\".')\n",
    "\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "    ## windows support (to do: linux and mac support)\n",
    "    if os == 'win64':\n",
    "        url = 'https://github.com/mozilla/geckodriver/releases/download/v0.30.0/geckodriver-v0.30.0-win64.zip'\n",
    "\n",
    "    if os == 'win32':\n",
    "        url = 'https://github.com/mozilla/geckodriver/releases/download/v0.30.0/geckodriver-v0.30.0-win32.zip'\n",
    "\n",
    "    ## download geckodriver\n",
    "    request = rq.get(\n",
    "        url = url\n",
    "    )\n",
    "\n",
    "    if request.status_code == 200:\n",
    "        gecko = zp.ZipFile(\n",
    "            file = io.BytesIO(request.content)\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError('Could not download GeckoDriver.')\n",
    "\n",
    "    try:\n",
    "        ## extract geckodriver\n",
    "        gecko.extractall(\n",
    "            path = path\n",
    "        )\n",
    "        \n",
    "        ## complete\n",
    "        gecko.close()\n",
    "\n",
    "        print('Successfully downloaded and extracted GeckoDriver.')\n",
    "\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    finally:\n",
    "        print('GeckoDriver detected. Proceeding to web requests.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "## make webdriver options\n",
    "def driver_options(opt):\n",
    "\n",
    "    \"\"\"\n",
    "    Desc:\n",
    "        Specifies Selenium WebDriver options for Firefox.\n",
    "\n",
    "    Args:\n",
    "        opt (list): Option string value flags (e.g. \"--headless\").\n",
    "\n",
    "    Returns:\n",
    "        FirefoxOptions: Selenium WebDriver options object for Firefox.\n",
    "\n",
    "    Raises:\n",
    "        TypeError: Incorrect data type in argument.\n",
    "    \"\"\"\n",
    "\n",
    "    ## arg quality\n",
    "    if type(opt) is not list:\n",
    "        raise TypeError('opt arg requires list of str.')\n",
    "    \n",
    "    else:\n",
    "        pass\n",
    "\n",
    "    ## firefox options\n",
    "    options = FirefoxOptions()\n",
    "\n",
    "    for i in opt:\n",
    "        options.add_argument(\n",
    "            argument = i\n",
    "        )\n",
    "\n",
    "    return options\n",
    "\n",
    "\n",
    "## make webdriver profile\n",
    "def driver_profile():\n",
    "\n",
    "    \"\"\"\n",
    "    Desc:\n",
    "        Creates Selenium WebDriver profile for Firefox.\n",
    "\n",
    "    Args:\n",
    "        None.\n",
    "\n",
    "    Returns:\n",
    "        FirefoxProfile: Selenium WebDriver profile object for Firefox.\n",
    "    \n",
    "    Raises:\n",
    "        None.\n",
    "    \"\"\"\n",
    "\n",
    "    ## firefox profile\n",
    "    profile = FirefoxProfile()\n",
    "\n",
    "    profile.set_preference(\n",
    "        key = 'browser.startup.homepage',\n",
    "        value = 'about.blank'\n",
    "    )\n",
    "\n",
    "    return profile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "## navigation by link anchors\n",
    "def link_clicker(driver, url, anch_x, n, t):\n",
    "\n",
    "    \"\"\" \n",
    "    Desc:\n",
    "        Navigates to link in website specified by anchor ID in 'anch_x'. Makes \n",
    "        'n' number of web request attempts before time out failure with 't' load \n",
    "        latency time. \n",
    "\n",
    "    Args:\n",
    "        url (str): URL of website.\n",
    "        driver (obj): Selenium WebDriver object.\n",
    "        anch_x (int): Anchor ID of link.\n",
    "        n (int): Number of web request attempts after first failure.\n",
    "        t (int): Load latency of website (seconds).\n",
    "    \n",
    "    Returns:\n",
    "        None\n",
    "    \n",
    "    Raises:\n",
    "        RuntimeError: Max 'n' number of attempts reached.\n",
    "    \"\"\"\n",
    "\n",
    "    ## get request url\n",
    "    driver.get(\n",
    "        url = url\n",
    "    )\n",
    "\n",
    "    ## load latency\n",
    "    wait = WebDriverWait(\n",
    "        driver = driver,\n",
    "        timeout = t\n",
    "    )\n",
    "\n",
    "    ## try n times\n",
    "    i = 1\n",
    "\n",
    "    ## nav to link\n",
    "    while i < n:\n",
    "        try:\n",
    "            branch = wait.until(\n",
    "                method = EC.element_to_be_clickable(\n",
    "                    locator = (By.ID, anch_x)\n",
    "                )\n",
    "            )\n",
    "            branch.click()\n",
    "            break\n",
    "\n",
    "        ## try again on failure\n",
    "        except:\n",
    "            i += 1\n",
    "            print('Unsuccessful request, trying again. Attempt: {x}'.format(\n",
    "                    x = i\n",
    "                )\n",
    "            )\n",
    "\n",
    "            ## get request url again\n",
    "            driver.get(\n",
    "                url = url\n",
    "            )\n",
    "            pass\n",
    "\n",
    "        ## time out failure on too many attempts\n",
    "        if n == i:\n",
    "            raise RuntimeError(\n",
    "                'Unsuccessful request, now stopping. Max number of attempts.'\n",
    "            )\n",
    "\n",
    "\n",
    "## name and section data\n",
    "def name_section_extractor(driver, anch_a, anch_b, anch_c, anch_d, n, t):\n",
    "\n",
    "    \"\"\"\n",
    "    Desc:\n",
    "        Primary function for retrieving raw data for 'Name', 'Education', and \n",
    "        'Section' features. Makes 'n' number of web request attempts before \n",
    "        time out failure with 't' load latency time. Anchor ID's 'anch_a', \n",
    "        'anch_b', 'anch_c', 'anch_d' pre-specified from known information \n",
    "        based on prior website inspection.\n",
    "\n",
    "    Args:\n",
    "        driver (obj): Selenium WebDriver object.\n",
    "        anch_a (str): Anchor ID of link.\n",
    "        anch_b (str): Anchor ID of link.\n",
    "        anch_c (str): Anchor ID of link.\n",
    "        anch_d (str): Anchor ID of link.\n",
    "        n (int): Number of web request attempts after first failure.\n",
    "        t (int): Load latency of website (seconds).\n",
    "\n",
    "    Returns:\n",
    "        list: List of lists containing strings of names and lab desc.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: Cannot split strings.\n",
    "        RuntimeError: Max 'n' number of attempts reached. \n",
    "    \"\"\"\n",
    "\n",
    "    ## load latency\n",
    "    wait = WebDriverWait(\n",
    "        driver = driver,\n",
    "        timeout = t\n",
    "    )\n",
    "\n",
    "    ## try n times\n",
    "    i = 1\n",
    "\n",
    "    ## nav to link\n",
    "    while i < n:\n",
    "        try:\n",
    "\n",
    "            ## -- multiple researcher profiles -- ##\n",
    "            ## contains branch and section/unit columns\n",
    "            try:\n",
    "\n",
    "                ## global web elements\n",
    "                element_all = wait.until(\n",
    "                    method = EC.presence_of_element_located(\n",
    "                        locator = (By.XPATH, anch_a)\n",
    "                    )\n",
    "                )\n",
    "\n",
    "                ## global list\n",
    "                element_all_lst = element_all.find_elements_by_tag_name(\n",
    "                    name = \"li\"\n",
    "                )\n",
    "\n",
    "                people_all = list()\n",
    "\n",
    "                for i in element_all_lst:\n",
    "                    people_all.append(i.text)\n",
    "\n",
    "                ## global list to contain only names, edu, section/unit\n",
    "                people_all = [i for i in people_all if \"\\n\" in i]\n",
    "\n",
    "                ## subset web elements\n",
    "                element_sub = wait.until(\n",
    "                    method = EC.presence_of_element_located(\n",
    "                        locator = (By.XPATH, anch_b)\n",
    "                    )\n",
    "                )\n",
    "\n",
    "                ## subset web element list\n",
    "                element_sub_lst = element_sub.find_elements_by_tag_name(\n",
    "                    name = \"li\"\n",
    "                )\n",
    "\n",
    "                people_sub = list()\n",
    "\n",
    "                for i in element_sub_lst:\n",
    "                    people_sub.append(i.text)\n",
    "\n",
    "                ## subset web element list to contain only names, edu, section/unit\n",
    "                people_sub = [i for i in people_sub if \"\\n\" in i]\n",
    "\n",
    "                ## section/unit list\n",
    "                n = len(people_sub)\n",
    "                people_sec = people_all[n:]\n",
    "\n",
    "                people_sec_spt = list()\n",
    "                n = len(people_sec)\n",
    "\n",
    "                for i in range(0, n):\n",
    "                    try:\n",
    "                        people_sec_spt.append(\n",
    "                            [people_sec[i].split('\\n')[1], people_sec[i].split('\\n')[0]]\n",
    "                        )\n",
    "\n",
    "                    except ValueError as err:\n",
    "                        print(err.args, 'Cannot split strings in Sections and Units element.')\n",
    "\n",
    "                ## branch list\n",
    "                people_sub_spt = list()\n",
    "                n = len(people_sub)\n",
    "\n",
    "                for i in range(0, n):\n",
    "                    try:\n",
    "                        people_sub_spt.append(\n",
    "                            people_sub[i].split('\\n')\n",
    "                        )\n",
    "                    except ValueError as err:\n",
    "                        print(err.args, 'Cannot split strings in People element.')\n",
    "\n",
    "                ## combined branch and section/unit list\n",
    "                people_all_spt = people_sec_spt + people_sub_spt\n",
    "\n",
    "                ## remove duplicates\n",
    "                for i in people_all_spt:\n",
    "                    n = len(i)\n",
    "                    if n > 2:\n",
    "                        people_all_spt.remove(i)\n",
    "\n",
    "            ## -- single researcher profile -- ##\n",
    "            ## does not contain branch and section/unit columns\n",
    "            except:\n",
    "\n",
    "                ## name, edu, section/unit\n",
    "                people_all_spt = list()\n",
    "                string_all_spt = list()\n",
    "\n",
    "                ## name, section/unit text\n",
    "                anchors = [\n",
    "                    anch_c,\n",
    "                    anch_d\n",
    "                ]\n",
    "\n",
    "                n = len(anchors)\n",
    "\n",
    "                for i in range(0, n):\n",
    "                    element_all = wait.until(\n",
    "                        method = EC.presence_of_element_located(\n",
    "                            locator = (By.XPATH, anchors[i])\n",
    "                        )\n",
    "                    )\n",
    "                    string_all_spt.insert(i, element_all.find_element_by_xpath(\n",
    "                            xpath = anchors[i]\n",
    "                        ).text\n",
    "                    )\n",
    "\n",
    "                ## store list in list\n",
    "                people_all_spt.insert(0, string_all_spt)\n",
    "\n",
    "            finally:\n",
    "                return people_all_spt\n",
    "\n",
    "        ## try again on failure\n",
    "        except:\n",
    "            i += 1\n",
    "            print('Unsuccessful request to link, trying again. Attempt: {x}'.format(\n",
    "                    x = i\n",
    "                )\n",
    "            )\n",
    "            pass\n",
    "\n",
    "        ## time out failure on too many attempts\n",
    "        if n == i:\n",
    "            raise RuntimeError(\n",
    "                'Unsuccessful request, now stopping. Max number of attempts.'\n",
    "            )\n",
    "\n",
    "\n",
    "## name and section pre-processing\n",
    "def name_section_processor(data, feat_a, feat_b):\n",
    "\n",
    "    \"\"\"\n",
    "    Desc:\n",
    "        Addresses 'Section' feature. Subordinate function for determining\n",
    "        duplicate observations by values in 'feat_b' encoded with a special\n",
    "        character (comma). Removes redundant observations.\n",
    "\n",
    "    Args:\n",
    "        data (df): A valid DataFrame.\n",
    "        feat_a (str): Reference column, typically 'Name'.\n",
    "        feat_b (str): Modification column, typically 'Section'.\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame: Contains modified 'feat_b' feature.\n",
    "    \n",
    "    Raises:\n",
    "        None.\n",
    "    \"\"\"\n",
    "    \n",
    "    ## make researcher name and section feats\n",
    "    feats = [\n",
    "        feat_a,\n",
    "        feat_b,\n",
    "    ]\n",
    "\n",
    "    data = pd.DataFrame(\n",
    "        data = data,\n",
    "        columns = feats\n",
    "    )\n",
    "\n",
    "    ## remove multiple section names\n",
    "    return data[~data[feat_b].str.contains(',')]\n",
    "\n",
    "\n",
    "## name and education pre-processing\n",
    "def name_educat_processor(data, feat_a, feat_b):\n",
    "\n",
    "    \"\"\"\n",
    "    Desc:\n",
    "        Creates new 'Education' feature. Removes job title and family suffix \n",
    "        from values in 'feat_a'. Utilizes remaining substring values to move \n",
    "        specified education credentials to 'feat_b'. Duplicate values in \n",
    "        'feat_a' will occure where there are multiple education credentials.\n",
    "\n",
    "    Args:\n",
    "        data (df): A valid DataFrame.\n",
    "        feat_a (str): Reference column, typically 'Name'.\n",
    "        feat_b (str): Creation column, typically 'Education'.\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame: Contains new 'feat_b' feature.\n",
    "    \n",
    "    Raises:\n",
    "        None.\n",
    "    \"\"\"\n",
    "    \n",
    "    ## job title suffix\n",
    "    titles = [\n",
    "        'Chief',\n",
    "        'Director',\n",
    "        'Diplomate',\n",
    "        'Senior Investigator',\n",
    "        'Facility Veterinarian',\n",
    "        'FRCPA Staff Clinician',\n",
    "        'FRCPA',\n",
    "        'Diplomate ACLAM',\n",
    "        'ACLAM',\n",
    "        'FAAAAI',\n",
    "        'Acting',\n",
    "        'Associate',\n",
    "        'Staff Clinician'\n",
    "    ]\n",
    "\n",
    "    ## punctuation\n",
    "    puncs = [\n",
    "        ';',\n",
    "        ',',\n",
    "        '.'\n",
    "    ]\n",
    "\n",
    "    ## remove job title suffix and punctuation\n",
    "    remove = titles + puncs\n",
    "\n",
    "    for i in remove:\n",
    "        data[feat_a] = data[feat_a].str.replace(\n",
    "            pat = i,\n",
    "            repl = '',\n",
    "            case = True,\n",
    "            regex = False\n",
    "        )\n",
    "\n",
    "    ## remove leading and trailing whitespace\n",
    "    for i in data.columns:\n",
    "        data[i] = data[i].str.strip()\n",
    "\n",
    "    ## edu suffix\n",
    "    edu = [\n",
    "        'MA',\n",
    "        'MSc',\n",
    "        'MS',\n",
    "        'MHSc',\n",
    "        'MHS',\n",
    "        'MPVM',\n",
    "        'MPH',\n",
    "        'MD',\n",
    "        'ScD',\n",
    "        'DSc',\n",
    "        'DVM',\n",
    "        'DPhil',\n",
    "        'PhD',\n",
    "        'Dr rer nat'\n",
    "    ]\n",
    "\n",
    "    ## make edu feature\n",
    "    data.insert(\n",
    "        loc = 1,\n",
    "        column = feat_b,\n",
    "        value = None\n",
    "    )\n",
    "\n",
    "    ## move edu from suffix to edu feature\n",
    "    n = len(data)\n",
    "\n",
    "    for i in range(0, n):\n",
    "        for j in edu:\n",
    "            if j in data[feat_a].iloc[i]:\n",
    "                data[feat_a].iloc[i] = data[feat_a].iloc[i].replace(j, '')\n",
    "                data[feat_a].iloc[i] = data[feat_a].iloc[i].replace('  ', ' ')\n",
    "                if data[feat_b].iloc[i] is None:\n",
    "                    data[feat_b].iloc[i] = j\n",
    "                else:\n",
    "                    data = data.append(data.iloc[i])\n",
    "                    data[feat_b].iloc[i] = None\n",
    "                    data[feat_b].iloc[i] = j\n",
    "\n",
    "    ## assume credentials not listed\n",
    "    data[feat_b].fillna(\n",
    "        value = 'Other',\n",
    "        inplace = True\n",
    "    )\n",
    "\n",
    "    ## remove duplicate edu\n",
    "    for i in edu:\n",
    "        data[feat_a] = data[feat_a].str.replace(\n",
    "            pat = i,\n",
    "            repl = '',\n",
    "            case = True,\n",
    "            regex = False\n",
    "        )\n",
    "\n",
    "    ## remove parath\n",
    "    data[feat_a] = data[feat_a].str.replace(\n",
    "            pat = r\"\\(.*\\)\",\n",
    "            repl = '',\n",
    "            regex = True\n",
    "        )\n",
    "\n",
    "    ## remove leading and trailing whitespace\n",
    "    for i in data.columns:\n",
    "        data[i] = data[i].str.strip()\n",
    "\n",
    "    ## remove duplicate observations\n",
    "    data.drop_duplicates(\n",
    "        inplace = True\n",
    "    )\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "## branch data and pre-processing\n",
    "def branch_extractor(driver, data, feat_a, feat_b, anch_c, t):\n",
    "\n",
    "    \"\"\"\n",
    "    Desc:\n",
    "        Creates new 'Branch' feature. Utilizies web requests and DataFrame \n",
    "        referencing. For multiple researchers per 'feat_b', 'feat_a' will \n",
    "        contain different values than 'feat_b'. For one researcher per 'feat_b',\n",
    "        'feat_a' and 'feat_b' contain the same values.\n",
    "\n",
    "    Args:\n",
    "        driver (obj): Selenium WebDriver object.\n",
    "        data (df): A valid DataFrame.\n",
    "        feat_a (str): Reference column, typically 'Section'.\n",
    "        feat_b (str): Target column, typically 'Branch'.\n",
    "        anch_c (str): Anchor ID of link.\n",
    "        t (int): Load latency of website (seconds).\n",
    "\n",
    "    Returns:\n",
    "        Dataframe: Contains new 'feat_b' feature.\n",
    "    \n",
    "    Raises:\n",
    "        ValueError: Could not locate web resource.\n",
    "        ValueError: 'Branch' feature could not be created.\n",
    "    \"\"\"\n",
    "\n",
    "    ## load latency\n",
    "    wait = WebDriverWait(\n",
    "        driver = driver,\n",
    "        timeout = t\n",
    "    )\n",
    "\n",
    "    ## add feat\n",
    "    n = len(data)\n",
    "\n",
    "    ## multiple researchers\n",
    "    if n > 1:\n",
    "        try:\n",
    "            element = wait.until(\n",
    "                method = EC.presence_of_element_located(\n",
    "                    locator = (By.XPATH, anch_c)\n",
    "                )\n",
    "            )\n",
    "\n",
    "            feat_branch = element.find_element_by_xpath(\n",
    "                xpath = anch_c\n",
    "            ).text\n",
    "\n",
    "        except ValueError as err:\n",
    "            print(err.args, 'Cannot find heading.')\n",
    "\n",
    "    ## single researcher\n",
    "    elif n == 1:\n",
    "        feat_branch = data[feat_a].iloc[0]\n",
    "\n",
    "    else:\n",
    "        raise ValueError('Could not create branch name feature.')\n",
    "\n",
    "    data.insert(\n",
    "        loc = 2,\n",
    "        column = feat_b,\n",
    "        value = feat_branch\n",
    "    )\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "## modify section names\n",
    "def section_processor(data, feat_a, feat_b):\n",
    "\n",
    "    \"\"\"\n",
    "    Desc:\n",
    "        Addresses 'Section' feature. Modifies string values to observations in \n",
    "        'feat_a' by including the corresponding string values found in 'feat_b' \n",
    "        if there are duplicate values in 'feat_a', where the values found in \n",
    "        'feat_b' are different.\n",
    "\n",
    "    Args:\n",
    "        data (df): A valid DataFrame.\n",
    "        feat_a (str): Target column, typically 'Section'.\n",
    "        feat_b (str): Reference column, typically 'Branch'.\n",
    "\n",
    "    Returns:\n",
    "        Dataframe: Annotated observations in 'feat_a' column.\n",
    "    \n",
    "    Raises:\n",
    "        None.\n",
    "    \"\"\"\n",
    "\n",
    "    ## add parenth for matching sections with different branch\n",
    "    mix_sec = list()\n",
    "\n",
    "    for i in data[feat_a].unique():\n",
    "        if len(data[data[feat_a] == i][feat_b].unique()) > 1:\n",
    "            mix_sec.append(i)\n",
    "\n",
    "    for i in mix_sec:\n",
    "        data_sec = data[data[feat_a] == i].copy()\n",
    "        n = len(data_sec)\n",
    "        for j in range(0, n):\n",
    "            name_sec = (\n",
    "                data_sec[feat_a].iloc[j] + ' ' + '(' + data_sec[feat_b].iloc[j] + ')'\n",
    "            )\n",
    "            data_sec[feat_a].iloc[j] = name_sec\n",
    "\n",
    "        data.update(\n",
    "            other = data_sec\n",
    "        )\n",
    "\n",
    "    ## reindex data\n",
    "    data.reset_index(\n",
    "        drop = True,\n",
    "        inplace = True\n",
    "    )\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Namer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "## modify first names\n",
    "def first_namer(data, feat):\n",
    "\n",
    "    \"\"\"\n",
    "    Desc:\n",
    "        Addresses 'Name' feature. Performs ad hoc changes to abbreviated or \n",
    "        otherwise misspelled first names. This function should evolve when \n",
    "        errors in first names are recognized or improved info.\n",
    "     \n",
    "    Args:\n",
    "        data (df): A valid DataFrame.\n",
    "        feat (str): Target column, typically 'Name'.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: Cleaned first name substring values in 'feat' column.\n",
    "    \n",
    "    Raises:\n",
    "        None.\n",
    "    \"\"\"\n",
    "\n",
    "    ## individual name corrections\n",
    "    feat_repl = {\n",
    "\n",
    "        ## slight abuse of original purpose\n",
    "        ## info: https://ned.nih.gov/search/\n",
    "        feat: {\n",
    "            'Beth Fischer': 'Elizabeth Fischer',\n",
    "            'David Hackstadt': 'Ted Hackstadt'\n",
    "        }\n",
    "    }\n",
    "\n",
    "    data.replace(\n",
    "        to_replace = feat_repl,\n",
    "        inplace = True\n",
    "    )\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "## modify middle names\n",
    "def middle_namer(data, feat):\n",
    "\n",
    "    \"\"\"\n",
    "    Desc:\n",
    "        Addresses 'Name' feature. Inserts middle initial or name into string\n",
    "        values that have matching first and last names when compared to other\n",
    "        observation, but where the middle initial or name is absent.\n",
    "     \n",
    "    Args:\n",
    "        data (df): A valid DataFrame.\n",
    "        feat (str): Target column, typically 'Name'.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: Cleaned middle name substring values in 'feat' column.\n",
    "    \n",
    "    Raises:\n",
    "        None.\n",
    "    \"\"\"\n",
    "\n",
    "    ## clean names\n",
    "    n = len(data) - 1\n",
    "\n",
    "    for i in range(0, n):\n",
    "\n",
    "        ## use full name containing middle initial\n",
    "        name_one = str(\n",
    "            data[feat].iloc[i].split()[0] + ' ' + \n",
    "            data[feat].iloc[i].split()[-1]\n",
    "        )\n",
    "\n",
    "        name_two = str(\n",
    "            data[feat].iloc[i + 1].split()[0] + ' ' + \n",
    "            data[feat].iloc[i + 1].split()[-1]\n",
    "        )\n",
    "\n",
    "        if name_one == name_two:\n",
    "            n_name_one = len(data[feat].iloc[i].split())\n",
    "            n_name_two = len(data[feat].iloc[i + 1].split())\n",
    "\n",
    "            if n_name_one != n_name_two:\n",
    "                if n_name_one > 2:\n",
    "                    name_use = data[feat].iloc[i]\n",
    "                if n_name_two > 2:\n",
    "                    name_use = data[feat].iloc[i + 1]\n",
    "                else:\n",
    "                    pass\n",
    "\n",
    "                data[feat].iloc[i] = name_use\n",
    "                data[feat].iloc[i + 1] = name_use\n",
    "\n",
    "            else:\n",
    "                pass\n",
    "\n",
    "        else:\n",
    "            pass\n",
    "    \n",
    "    ## individual name corrections\n",
    "    feat_repl = {\n",
    "\n",
    "        ## slight abuse of original purpose\n",
    "        ## info: https://ned.nih.gov/search/\n",
    "        feat: {\n",
    "            'Elizabeth Fischer': 'Elizabeth R Fischer', \n",
    "            'David Sacks': 'David L Sacks',\n",
    "            'Daniella Schwartz': 'Daniella M Schwartz',\n",
    "            'Richard Davey': 'Richard T Davey',\n",
    "            'Louis Miller': 'Louis H Miller',\n",
    "            'Catharine Bosio': 'Catharine M Bosio'\n",
    "        }\n",
    "    }\n",
    "\n",
    "    data.replace(\n",
    "        to_replace = feat_repl,\n",
    "        inplace = True\n",
    "    )\n",
    "\n",
    "    ## remove duplicates\n",
    "    data.drop_duplicates(\n",
    "        inplace = True\n",
    "    )\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "## modify last names\n",
    "def last_namer(data, feat, reap = 3):\n",
    "\n",
    "    \"\"\"\n",
    "    Desc:\n",
    "        Addresses 'Name' feature. Modifies last name substring by correcting \n",
    "        errors when misspellings are assumed to be missing letters. Replaces \n",
    "        assumed misspelling with string value of greater length. Also removes \n",
    "        family name suffix.\n",
    "     \n",
    "    Args:\n",
    "        data (df): A valid DataFrame.\n",
    "        feat (str): Target column, typically 'Name'.\n",
    "        reap (int): Number of substrings in last name to compare.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: Cleaned last name substring values in 'feat' column.\n",
    "    \n",
    "    Raises:\n",
    "        None.\n",
    "    \"\"\"\n",
    "\n",
    "    ## clean names\n",
    "    n = len(data) - 1\n",
    "\n",
    "    for i in range(0, n):\n",
    "\n",
    "        ## remove suffix from last names\n",
    "        name_sir = data[feat].iloc[i].split()[-1]\n",
    "        n_name_sir = len(name_sir)\n",
    "\n",
    "        ## remove suffix errors\n",
    "        if n_name_sir == 1:\n",
    "            data[feat].iloc[i] = data[feat].iloc[i][:-2]\n",
    "\n",
    "        ## remove family suffix\n",
    "        if n_name_sir <= 3:\n",
    "            fam_suf = [\n",
    "                'III',\n",
    "                'II',\n",
    "                'Jr',\n",
    "                'Sr'\n",
    "            ]\n",
    "\n",
    "            for j in fam_suf:\n",
    "                if name_sir == j:\n",
    "                    data[feat].iloc[i] = ' '.join(data[feat].iloc[i].split()[:-1])\n",
    "\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "        ## replace missing letters in last names \n",
    "        ## assumes same last name for first 'reap' repeated letters\n",
    "        name_giv_one = data[feat].iloc[i].split()[0]\n",
    "        name_giv_two = data[feat].iloc[i + 1].split()[0]\n",
    "\n",
    "        name_sir_one = data[feat].iloc[i].split()[-1]\n",
    "        name_sir_two = data[feat].iloc[i + 1].split()[-1]\n",
    "\n",
    "        if name_giv_one == name_giv_two and name_sir_one[0:reap] == name_sir_two[0:reap]:\n",
    "            n_name_sir_one = len(name_sir_one)\n",
    "            n_name_sir_two = len(name_sir_two)\n",
    "\n",
    "            if n_name_sir_one != n_name_sir_two:\n",
    "                if n_name_sir_one > n_name_sir_two:\n",
    "                    data[feat].iloc[i + 1] = data[feat].iloc[i]\n",
    "\n",
    "                if n_name_sir_one < n_name_sir_two:\n",
    "                    data[feat].iloc[i] = data[feat].iloc[i + 1]\n",
    "\n",
    "            else:\n",
    "                pass\n",
    "\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "\n",
    "    ## individual name corrections\n",
    "    feat_repl = {\n",
    "\n",
    "        ## slight abuse of original purpose\n",
    "        ## info: https://ned.nih.gov/search/\n",
    "        feat: {\n",
    "            'Jennifer M Cuellar-Rodriguez': 'Jennifer M Cuellar-RodrÃ­guez',\n",
    "            'Sumati Ragagopalan': 'Sumati Rajagopalan'\n",
    "        }\n",
    "    }\n",
    "\n",
    "    data.replace(\n",
    "        to_replace = feat_repl,\n",
    "        inplace = True\n",
    "    )\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "## naming wrapper\n",
    "def name_processor(data, feat, reap):\n",
    "\n",
    "    \"\"\"\n",
    "    Desc:\n",
    "        Addresses 'Name' feature. Unifies subordinate naming functions for \n",
    "        string and substring values. Utilized for later implementation in\n",
    "        web scraping procedure.\n",
    "     \n",
    "    Args:\n",
    "        data (df): A valid DataFrame.\n",
    "        feat (str): Target column, typically 'Name'.\n",
    "        reap (int): Number of characters to compare in last name substring.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: Cleaned name string values in 'feat' column.\n",
    "    \n",
    "    Raises:\n",
    "        None.\n",
    "    \"\"\"\n",
    "\n",
    "    ## remove whitespace\n",
    "    data[feat] = data[feat].str.strip()\n",
    "\n",
    "    data.replace(\n",
    "        to_replace = {' +':' '},\n",
    "        regex = True,\n",
    "        inplace = True\n",
    "    )\n",
    "\n",
    "    ## sort data by names\n",
    "    data.sort_values(\n",
    "        by = feat,\n",
    "        ascending = False,\n",
    "        inplace = True\n",
    "    )\n",
    "    \n",
    "    ## first\n",
    "    data = first_namer(\n",
    "        data = data,\n",
    "        feat = feat\n",
    "    )\n",
    "\n",
    "    ## middle\n",
    "    data = middle_namer(\n",
    "        data = data,\n",
    "        feat = feat\n",
    "    )\n",
    "\n",
    "    ## last\n",
    "    data = last_namer(\n",
    "        data = data,\n",
    "        feat = feat,\n",
    "        reap = reap  ## num char to compare\n",
    "    )\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scraper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "## web scaper\n",
    "def data_scraper(feats, driver, url, anch_a = 354, anch_b = 374, n = 3, t = 30):\n",
    "\n",
    "    \"\"\"\n",
    "    Desc:\n",
    "        Primary web scraping function. Unifies all subordinate functions to \n",
    "        traverses website through anchor ID's and pre-process the data returned \n",
    "        in a DataFrame.\n",
    "\n",
    "    Args:\n",
    "        feats (tuple): Columns as string values.\n",
    "        driver (obj): Selenium WebDriver object.\n",
    "        url (str): URL of website.\n",
    "        anch_a (int): Anchor ID at start of website traversal.\n",
    "        anch_b (int): Anchor ID at end of website traversal.\n",
    "        n (int): Number of web request attempts after first failure.\n",
    "        t (int): Load latency of website (seconds).\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: 'Name', 'Education', 'Branch', 'Section' features.\n",
    "    \n",
    "    Raises:\n",
    "        TypeError: Incorrect data type in an argument.\n",
    "    \"\"\"\n",
    "\n",
    "    ## arg quality\n",
    "    if type(feats) is not tuple:\n",
    "        raise TypeError('feats arg requires tuple of str.')\n",
    "    \n",
    "    if driver is None:\n",
    "        raise TypeError('driver arg requires Selenium WebDriver object.')\n",
    "    \n",
    "    if type(url) is not str:\n",
    "        raise TypeError('url arg requires a valid str.')\n",
    "    \n",
    "    if type(anch_a) is not int:\n",
    "        raise TypeError('url arg requires a pos int within anchor ID range.')\n",
    "\n",
    "    if type(anch_b) is not int:\n",
    "        raise TypeError('url arg requires a pos int within anchor ID range.')\n",
    "    \n",
    "    if type(n) is not int or t < 1:\n",
    "        raise TypeError('n arg requires a pos int.')\n",
    "    \n",
    "    if type(t) is not int or t < 1:\n",
    "        raise TypeError('t arg requires a pos int.')\n",
    "    \n",
    "    else:\n",
    "        pass\n",
    "\n",
    "    ## make dataframe\n",
    "    feats = list(feats)\n",
    "    data = pd.DataFrame(\n",
    "        columns = feats\n",
    "    )\n",
    "\n",
    "    ## traverse lab desc\n",
    "    for i in tqdm(range(anch_a, anch_b), \n",
    "        ascii = True, \n",
    "        desc = \"Scraping Data from NIAID DIR Laboratory Descriptions\"\n",
    "        ):\n",
    "\n",
    "        ## link nav\n",
    "        link_clicker(\n",
    "            url = url,\n",
    "            driver = driver,\n",
    "            anch_x =  'anch_{x}'.format(x = i),\n",
    "            n = n,\n",
    "            t = t\n",
    "        )\n",
    "\n",
    "        ## -- name, edu, section -- ##\n",
    "        ## name and section raw extraction\n",
    "        data_list = name_section_extractor(\n",
    "            driver = driver,\n",
    "            anch_a = '//*[@class=\"block block-layout-builder block-field-blocknodedivisionfield-subtopic-division\"]',\n",
    "            anch_b = '//*[@class=\"clearfix text-formatted field field--name-field-body field--type-text-long field--label-hidden field__item\"]',\n",
    "            anch_c = '//h1',\n",
    "            anch_d = '//*[@id=\"anch_346\"]',\n",
    "            n = n,\n",
    "            t = t\n",
    "        )\n",
    "\n",
    "        ## name and section feat processing\n",
    "        data_loop = name_section_processor(\n",
    "            data = data_list,\n",
    "            feat_a = feats[0],\n",
    "            feat_b = feats[3]\n",
    "        )\n",
    "\n",
    "        ## education feat processing\n",
    "        data_loop = name_educat_processor(\n",
    "            data = data_loop,\n",
    "            feat_a = feats[0],\n",
    "            feat_b = feats[1]\n",
    "        )\n",
    "\n",
    "        ## -- branch and section -- ##\n",
    "        # branch raw extraction\n",
    "        data_loop = branch_extractor(\n",
    "            driver = driver,\n",
    "            data = data_loop,\n",
    "            feat_a = feats[3],\n",
    "            feat_b = feats[2],\n",
    "            anch_c = '//h1',\n",
    "            t = t\n",
    "        )\n",
    "\n",
    "        ## -- global -- ##\n",
    "        ## create data\n",
    "        data = data.append(\n",
    "            other = data_loop,\n",
    "            ignore_index = True\n",
    "        )\n",
    "\n",
    "    ## name feat processing\n",
    "    data = name_processor(\n",
    "        data = data,\n",
    "        feat = feats[0],\n",
    "        reap = 2\n",
    "    )\n",
    "\n",
    "    ## branch and section processing\n",
    "    data = section_processor(\n",
    "        data = data,\n",
    "        feat_a = feats[3],\n",
    "        feat_b = feats[2]\n",
    "    )\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "## data wrangling\n",
    "def data_cleaner(data, feats):\n",
    "\n",
    "    \"\"\"\n",
    "    Desc:\n",
    "        Utilized for global data cleaning results from web scraping and data\n",
    "        manipulation. Removes whitespace, sorts, and resets index in DataFrame.\n",
    "    \n",
    "    Args:\n",
    "        data (df): A valid DataFrame.\n",
    "        feats (list): List of column names as string values.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: Cleaned and sorted assumed for export to disk.\n",
    "    \n",
    "    Raises:\n",
    "        None.\n",
    "    \"\"\"\n",
    "\n",
    "    ## remove whitespace\n",
    "    for i in data.columns:\n",
    "        data[i] = data[i].str.strip()\n",
    "\n",
    "    data.replace(\n",
    "        to_replace = {' +':' '},\n",
    "        regex = True,\n",
    "        inplace = True\n",
    "    )\n",
    "\n",
    "    ## arbitrary sorting\n",
    "    data.sort_values(\n",
    "        by = [feats[2], feats[0]],\n",
    "        inplace = True\n",
    "    )\n",
    "\n",
    "    ## reset index\n",
    "    data.reset_index(\n",
    "        drop = True,\n",
    "        inplace = True\n",
    "    )\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GeckoDriver detected. Proceeding to web requests.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping Data from NIAID DIR Laboratory Descriptions:  35%|###5      | 7/20 [00:42<01:00,  4.69s/it]"
     ]
    }
   ],
   "source": [
    "## -- download geckodriver -- ##\n",
    "gecko_downloader(os = 'win64')\n",
    "\n",
    "## -- local machine input -- ##\n",
    "exe_path = './geckodriver.exe'\n",
    "csv_path = '../data/niaid-dir-org.csv'\n",
    "\n",
    "## webdriver settings\n",
    "option_flags = [\n",
    "    '--headless',\n",
    "    '--no-sandbox',\n",
    "    '--start-maximized',\n",
    "    '--ignore-certificate-errors',\n",
    "    '--disable-extensions'\n",
    "]\n",
    "\n",
    "options = driver_options(\n",
    "    opt = option_flags\n",
    ")\n",
    "\n",
    "profile = driver_profile()\n",
    "\n",
    "## webdriver initalization\n",
    "driver = webdriver.Firefox(\n",
    "    executable_path = exe_path,\n",
    "    firefox_profile = profile,\n",
    "    options = options\n",
    ")\n",
    "\n",
    "## data features *strictly* named and ordered\n",
    "feats = tuple((\n",
    "    'Name',\n",
    "    'Education',\n",
    "    'Branch',\n",
    "    'Section'\n",
    "    )\n",
    ")\n",
    "\n",
    "## web scraping and processing\n",
    "data = data_scraper(\n",
    "    feats = feats,\n",
    "    driver = driver,\n",
    "    url = 'https://www.niaid.nih.gov/research/division-intramural-research-labs',\n",
    "    t = 12\n",
    ")\n",
    "\n",
    "## data processing\n",
    "data = data_cleaner(\n",
    "    data = data,\n",
    "    feats = feats\n",
    ")\n",
    "\n",
    "## data preview\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exporter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## data to disk\n",
    "data.to_csv(\n",
    "    path_or_buf = csv_path,\n",
    "    index = False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### End"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3.9.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
